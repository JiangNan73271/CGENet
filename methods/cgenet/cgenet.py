import abc
import logging

import timm
import torch
import torch.nn as nn
import torch.nn.functional as F


from methods.backbone.pvt_v2_eff import pvt_v2_eff_b1, pvt_v2_eff_b2, pvt_v2_eff_b3, pvt_v2_eff_b4, pvt_v2_eff_b5
from methods.cgenet.ops import ConvBNReLU, PixelNormalizer, resize_to
from utils.box.CT import RDSP
from utils.box.GLP import CoordAtt, conv3x3_bn_relu
from .layers import SimpleASPP

from utils.box.loss_function.utils import get_coef, cal_ual
from methods.cgenet.enhanced_layers import MultiHeadMyNet


LOGGER = logging.getLogger("main")


class _CGENet_Base(nn.Module):

    @abc.abstractmethod
    def body(self):
        pass

    def forward(self, data, iter_percentage=1, **kwargs):
        if self.training:
            # è®­ç»ƒæ¨¡å¼ï¼šè·å–å¤šå±‚é¢„æµ‹ç»“æœ
            predictions = self.body(data=data)  # è¿”å› (main_out, p1, p2, p3)
            out = predictions
            mask = data["mask"]
            prob = out.sigmoid()

            # ğŸ”§ æ·»åŠ NaNæ£€æµ‹å’Œå¤„ç†ï¼Œé˜²æ­¢æ±¡æŸ“æ•´ä¸ªç½‘ç»œ
            def safe_loss(loss_tensor, loss_name, fallback_value=0.0):
                """å®‰å…¨çš„æŸå¤±å¤„ç†ï¼Œæ£€æµ‹å¹¶å¤„ç†NaN"""
                if torch.isnan(loss_tensor).any() or torch.isinf(loss_tensor).any():
                    print(f"è­¦å‘Š: {loss_name} åŒ…å«NaN/Infï¼Œä½¿ç”¨fallbackå€¼ {fallback_value}")
                    return torch.tensor(fallback_value, device=loss_tensor.device, requires_grad=True)
                return loss_tensor


        else:
            # æ¨ç†æ¨¡å¼ï¼šåªè·å–ä¸»é¢„æµ‹
            out = self.body(data=data)
            return out

        if self.training:

            def iou_loss(pred, mask):
                pred_sigmoid = torch.sigmoid(pred)
                inter = (pred_sigmoid * mask).sum(dim=(2, 3))
                union = (pred_sigmoid + mask).sum(dim=(2, 3))
                iou = 1 - (inter + 1) / (union - inter + 1)
                return iou.mean()

            def structure_loss(pred, mask):
                # é™åˆ¶é¢„æµ‹å€¼èŒƒå›´ï¼Œé˜²æ­¢æç«¯logitså¯¼è‡´æ•°å€¼é—®é¢˜
                pred = torch.clamp(pred, min=-10.0, max=10.0)

                # è®¡ç®—æƒé‡ï¼šè¾¹ç¼˜åŒºåŸŸæƒé‡æ›´é«˜
                weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)

                # åŠ æƒBCEæŸå¤±
                wbce = F.binary_cross_entropy_with_logits(pred, mask, reduction='none')
                # é˜²æ­¢é™¤é›¶
                weit_sum = weit.sum(dim=(2, 3)).clamp(min=1e-7)
                wbce = (weit * wbce).sum(dim=(2, 3)) / weit_sum

                # åŠ æƒIOUæŸå¤±
                pred_sigmoid = torch.sigmoid(pred)
                inter = ((pred_sigmoid * mask) * weit).sum(dim=(2, 3))
                union = ((pred_sigmoid + mask) * weit).sum(dim=(2, 3))
                # é˜²æ­¢é™¤é›¶å’Œæ•°å€¼ä¸ç¨³å®š
                union_safe = union - inter + 1e-7
                wiou = 1 - (inter + 1e-7) / union_safe

                # æ£€æŸ¥ä¸­é—´ç»“æœ
                if torch.isnan(wbce).any() or torch.isnan(wiou).any():
                    print("è­¦å‘Š: structure_losså†…éƒ¨è®¡ç®—å‡ºç°NaN")
                    return torch.tensor(0.0, device=pred.device, requires_grad=True)

                return (wbce + wiou).mean()

            losses = []
            loss_str = []


            if self.use_structure_loss:
                # ä¸»è¦ç»“æ„åŒ–æŸå¤±
                main_loss = structure_loss(out, mask)
                main_loss = safe_loss(main_loss, "main_structure_loss")
                losses.append(main_loss)
                loss_str.append(f"main_struct: {main_loss.item():.5f}")

            else:
                # ä¸»è¦BCEæŸå¤±
                main_bce = F.binary_cross_entropy_with_logits(input=out, target=mask, reduction="mean")
                main_bce = safe_loss(main_bce, "main_bce_loss")
                losses.append(main_bce)
                loss_str.append(f"main_bce: {main_bce.item():.5f}")

            ual_coef = get_coef(iter_percentage=iter_percentage, method='cos')
            ual_loss = cal_ual(seg_logits=out, seg_gts=mask)  # ä½¿ç”¨logitsè€Œä¸æ˜¯prob
            ual_loss *= ual_coef
            ual_loss = safe_loss(ual_loss, "ual_loss")  # å®‰å…¨å¤„ç†
            losses.append(ual_loss)
            loss_str.append(f"powual_{ual_coef:.5f}: {ual_loss.item():.5f}")

            vis_dict = {
                "sal": prob,
            }

            return dict(vis=vis_dict, loss=sum(losses), loss_str=" ".join(loss_str))
        else:
            return out

    def get_grouped_params(self):
        param_groups = {"pretrained": [], "fixed": [], "retrained": []}
        for name, param in self.named_parameters():
            if name.startswith("encoder.patch_embed1."):
                param.requires_grad = False
                param_groups["fixed"].append(param)
            elif name.startswith("encoder."):
                param_groups["pretrained"].append(param)
            else:
                if "clip." in name:
                    param.requires_grad = False
                    param_groups["fixed"].append(param)
                else:
                    param_groups["retrained"].append(param)
        LOGGER.info(
            f"Parameter Groups:{{"
            f"Pretrained: {len(param_groups['pretrained'])}, "
            f"Fixed: {len(param_groups['fixed'])}, "
            f"ReTrained: {len(param_groups['retrained'])}}}"
        )
        return param_groups


class RN50_CGENet(_CGENet_Base):
    def __init__(
            self, pretrained=True, num_frames=1, input_norm=True, mid_dim=64, siu_groups=4, hmu_groups=6,
            use_checkpoint=False,
            use_structure_loss=True,
            loss_weights=None,  # ğŸ”§ æŸå¤±æƒé‡é…ç½®
            **kwargs
    ):
        super().__init__()
        self.encoder = timm.create_model(
            model_name="resnet50", features_only=True, out_indices=range(5), pretrained=False
        )
        if pretrained:
            # ğŸ”§ ä»æœ¬åœ°è·¯å¾„åŠ è½½é¢„è®­ç»ƒæƒé‡
            local_weight_path = "/home/ygq/wyh/CGENet/pretrained_weight/resnet50-timm.pth"
            params = torch.load(local_weight_path, map_location="cpu")
            self.encoder.load_state_dict(params, strict=False)

        # ğŸ”§ æŸå¤±å‡½æ•°é…ç½®
        self.use_structure_loss = use_structure_loss
        if loss_weights is None:
            # é»˜è®¤æƒé‡é…ç½®ï¼ˆåŸºäºè®­ç»ƒæ—¥å¿—åˆ†æï¼‰
            self.loss_weights = {
                'bound': 1.0,
                'structure': 1.0,
                'bce': 1.0,  # BCEæŸå¤±æƒé‡ï¼ˆå½“ä¸ä½¿ç”¨ç»“æ„åŒ–æŸå¤±æ—¶ï¼‰
                'iou': 1.0,  # IOUæŸå¤±æƒé‡ï¼ˆå½“ä¸ä½¿ç”¨ç»“æ„åŒ–æŸå¤±æ—¶ï¼‰
            }
        else:
            self.loss_weights = loss_weights

        # self.tra_5 = SimpleASPP(self.embed_dims[3], out_dim=mid_dim)
        self.tra_5 = ConvBNReLU(2048, mid_dim, 3, 1, 1)
        self.tra_4 = ConvBNReLU(1024, mid_dim, 3, 1, 1)
        self.tra_3 = ConvBNReLU(512, mid_dim, 3, 1, 1)
        self.tra_2 = ConvBNReLU(256, mid_dim, 3, 1, 1)
        self.tra_1 = ConvBNReLU(64, mid_dim, 3, 1, 1)

        self.normalizer = PixelNormalizer() if input_norm else nn.Identity()
        self.predictor = nn.Sequential(
            nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False),
            ConvBNReLU(64, 32, 3, 1, 1),
            nn.Conv2d(32, 1, 1),
        )

        self.siu_5 = MultiHeadMyNet(mid_dim,num_heads=4)
        self.siu_4 = MultiHeadMyNet(mid_dim,num_heads=4)
        self.siu_3 = MultiHeadMyNet(mid_dim,num_heads=4)
        self.siu_2 = MultiHeadMyNet(mid_dim,num_heads=4)
        self.siu_1 = MultiHeadMyNet(mid_dim,num_heads=4)

        # #Decoder
        # self.Up1 = up_conv(ch_in=mid_dim, ch_out=mid_dim)
        # self.Up2 = up_conv(ch_in=mid_dim, ch_out=mid_dim)
        # self.Up3 = up_conv(ch_in=mid_dim, ch_out=mid_dim)
        # self.Up4 = up_conv(ch_in=mid_dim, ch_out=mid_dim)
        #
        # # ğŸ”§ FRDSPèåˆæ„ŸçŸ¥è§£ç å™¨æ¨¡å— - é›†æˆfusion_convå’ŒRDSPçš„ä¼˜åŠ¿
        # # è¾“å…¥é€šé“æ•°ä¸º mid_dim*2 (ä¸Šé‡‡æ ·ç‰¹å¾ + è·³è·ƒè¿æ¥ç‰¹å¾)
        # # è¾“å‡ºé€šé“æ•°ä¸º mid_dim
        # self.FRDSP1 = FRDSP(mid_dim * 2, mid_dim)  # ç¬¬ä¸€å±‚è§£ç èåˆ
        # self.FRDSP2 = FRDSP(mid_dim * 2, mid_dim)  # ç¬¬äºŒå±‚è§£ç èåˆ
        # self.FRDSP3 = FRDSP(mid_dim * 2, mid_dim)  # ç¬¬ä¸‰å±‚è§£ç èåˆ
        # self.FRDSP4 = FRDSP(mid_dim * 2, mid_dim)  # ç¬¬å››å±‚è§£ç èåˆ
        #
        # self.SCF = SimpleConvFusion(mid_dim * 2, mid_dim)
        #
        # # ğŸ”§ ä¼˜åŒ–çš„FPNç‰¹å¾ä¼ é€’é—¨æ§æœºåˆ¶
        # # ä¸ºè·¨å°ºåº¦ç‰¹å¾ä¼ é€’æ·»åŠ å¯å­¦ä¹ æƒé‡ï¼Œé¿å…è¿‡å¼ºæ³¨å…¥
        # self.fpn_gate_1_2 = nn.Parameter(torch.ones(1))  # f1 -> f2 çš„æƒé‡é—¨æ§
        # self.fpn_gate_1_3 = nn.Parameter(torch.ones(1))  # f1 -> f3 çš„æƒé‡é—¨æ§
        # self.fpn_gate_1_4 = nn.Parameter(torch.ones(1))  # f1 -> f4 çš„æƒé‡é—¨æ§
        # self.fpn_gate_1_5 = nn.Parameter(torch.ones(1))  # f1 -> f5 çš„æƒé‡é—¨æ§
        # self.fpn_gate_2_3 = nn.Parameter(torch.ones(1))  # f2 -> f3 çš„æƒé‡é—¨æ§
        # self.fpn_gate_2_4 = nn.Parameter(torch.ones(1))  # f2 -> f4 çš„æƒé‡é—¨æ§
        # self.fpn_gate_2_5 = nn.Parameter(torch.ones(1))  # f2 -> f5 çš„æƒé‡é—¨æ§
        # self.fpn_gate_3_4 = nn.Parameter(torch.ones(1))  # f3 -> f4 çš„æƒé‡é—¨æ§
        # self.fpn_gate_3_5 = nn.Parameter(torch.ones(1))  # f3 -> f5 çš„æƒé‡é—¨æ§
        # self.fpn_gate_4_5 = nn.Parameter(torch.ones(1))  # f4 -> f5 çš„æƒé‡é—¨æ§
        #
        # # FPNç‰¹å¾å¯¹é½å±‚ï¼ˆå¯é€‰ï¼Œå½“éœ€è¦æ›´å¥½çš„ç‰¹å¾å¯¹é½æ—¶ä½¿ç”¨ï¼‰
        # self.fpn_align_1_2 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)
        # self.fpn_align_1_3 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)
        # self.fpn_align_1_4 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)
        # self.fpn_align_1_5 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)
        # self.fpn_align_2_3 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)
        # self.fpn_align_2_4 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)
        # self.fpn_align_2_5 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)
        # self.fpn_align_3_4 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)
        # self.fpn_align_3_5 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)
        # self.fpn_align_4_5 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)

    def normalize_encoder(self, x):
        x = self.normalizer(x)
        c1, c2, c3, c4, c5 = self.encoder(x)
        return c1, c2, c3, c4, c5

    def body(self, data):
        l_trans_feats = self.normalize_encoder(data["image_l"])  # [8, 3, 576, 576]
        m_trans_feats = self.normalize_encoder(data["image_m"])  # [8, 3, 480, 480]
        s_trans_feats = self.normalize_encoder(data["image_s"])  # [8, 3, 384, 384]

        l1, m1, s1 = self.tra_5(l_trans_feats[4]), self.tra_5(m_trans_feats[4]), self.tra_5(s_trans_feats[4])
        f1 = self.siu_5(l=l1, m=m1, s=s1)  # [8, 64, 12, 12] (mid_dim)
        l2, m2, s2 = self.tra_4(l_trans_feats[3]), self.tra_4(m_trans_feats[3]), self.tra_4(s_trans_feats[3])
        f2 = self.siu_4(l=l2, m=m2, s=s2)  # [8, 64, 24, 24] (mid_dim)
        l3, m3, s3 = self.tra_3(l_trans_feats[2]), self.tra_3(m_trans_feats[2]), self.tra_3(s_trans_feats[2])
        f3 = self.siu_3(l=l3, m=m3, s=s3)  # [8, 64, 48, 48] (mid_dim)
        l4, m4, s4 = self.tra_2(l_trans_feats[1]), self.tra_2(m_trans_feats[1]), self.tra_2(s_trans_feats[1])
        f4 = self.siu_2(l=l4, m=m4, s=s4)  # [8, 64, 96, 96] (mid_dim)
        l5, m5, s5 = self.tra_1(l_trans_feats[0]), self.tra_1(l_trans_feats[0]), self.tra_1(s_trans_feats[0])
        f5 = self.siu_1(l=l5, m=m5, s=s5)

        # # DecoderğŸ”§ ä¼˜åŒ–çš„FPNå¤šå°ºåº¦ç‰¹å¾èåˆ - åŠ å…¥é—¨æ§æœºåˆ¶ï¼Œæ›´é€‚åˆä¼ªè£…ç›®æ ‡æ£€æµ‹
        # # ä¼ªè£…ç›®æ ‡éœ€è¦å¤šå°ºåº¦ä¿¡æ¯è¿›è¡Œå‡†ç¡®æ£€æµ‹ï¼Œä½†éœ€è¦é¿å…è¿‡å¼ºçš„ç‰¹å¾æ³¨å…¥å¯¼è‡´ä¿¡æ¯å†²çª
        #
        # # å¤šå°ºåº¦æ’å€¼ï¼šå°†æ·±å±‚ç‰¹å¾ä¼ æ’­åˆ°æµ…å±‚ï¼Œå¹¶é€šè¿‡å¯¹é½å±‚è¿›è¡Œç‰¹å¾å¯¹é½
        # mf1_2 = F.interpolate(f1, size=f2.shape[-2:], mode='bilinear', align_corners=False)
        # mf1_2 = self.fpn_align_1_2(mf1_2)  # ç‰¹å¾å¯¹é½
        #
        # mf1_3 = F.interpolate(f1, size=f3.shape[-2:], mode='bilinear', align_corners=False)
        # mf1_3 = self.fpn_align_1_3(mf1_3)  # ç‰¹å¾å¯¹é½
        #
        # mf1_4 = F.interpolate(f1, size=f4.shape[-2:], mode='bilinear', align_corners=False)
        # mf1_4 = self.fpn_align_1_4(mf1_4)  # ç‰¹å¾å¯¹é½
        #
        # mf1_5 = F.interpolate(f1, size=f5.shape[-2:], mode='bilinear', align_corners=False)
        # mf1_5 = self.fpn_align_1_5(mf1_5)  # ç‰¹å¾å¯¹é½
        #
        # mf2_3 = F.interpolate(f2, size=f3.shape[-2:], mode='bilinear', align_corners=False)
        # mf2_3 = self.fpn_align_2_3(mf2_3)  # ç‰¹å¾å¯¹é½
        #
        # mf2_4 = F.interpolate(f2, size=f4.shape[-2:], mode='bilinear', align_corners=False)
        # mf2_4 = self.fpn_align_2_4(mf2_4)  # ç‰¹å¾å¯¹é½
        #
        # mf2_5 = F.interpolate(f2, size=f5.shape[-2:], mode='bilinear', align_corners=False)
        # mf2_5 = self.fpn_align_2_5(mf2_5)  # ç‰¹å¾å¯¹é½
        #
        # mf3_4 = F.interpolate(f3, size=f4.shape[-2:], mode='bilinear', align_corners=False)
        # mf3_4 = self.fpn_align_3_4(mf3_4)  # ç‰¹å¾å¯¹é½
        #
        # mf3_5 = F.interpolate(f3, size=f5.shape[-2:], mode='bilinear', align_corners=False)
        # mf3_5 = self.fpn_align_3_5(mf3_5)  # ç‰¹å¾å¯¹é½
        #
        # mf4_5 = F.interpolate(f4, size=f5.shape[-2:], mode='bilinear', align_corners=False)
        # mf4_5 = self.fpn_align_4_5(mf4_5)  # ç‰¹å¾å¯¹é½
        #
        # enhanced_f1 = f1  # æœ€æ·±å±‚ä¿æŒåŸæ ·
        # enhanced_f2 = f2 + self.fpn_gate_1_2 * mf1_2 # èåˆæ¥è‡ªf1çš„è¯­ä¹‰ä¿¡æ¯
        # enhanced_f3 = f3 + self.fpn_gate_1_3 * mf1_3 + self.fpn_gate_2_3 * mf2_3 # èåˆæ¥è‡ªf1,f2çš„å¤šå°ºåº¦ä¿¡æ¯
        # enhanced_f4 = f4 + self.fpn_gate_1_4 * mf1_4 + self.fpn_gate_2_4 * mf2_4 + self.fpn_gate_3_4 * mf3_4 # èåˆæ¥è‡ªf1,f2,f3çš„å¤šå°ºåº¦ä¿¡æ¯
        # enhanced_f5 = f5 + self.fpn_gate_1_5 * mf1_5 + self.fpn_gate_2_5 * mf2_5 + self.fpn_gate_3_5 * mf3_5 + self.fpn_gate_4_5 * mf4_5 # èåˆæ¥è‡ªf1,f2,f3,f4çš„å¤šå°ºåº¦ä¿¡æ¯
        #
        # # ğŸ”§ FRDSPèåˆæ„ŸçŸ¥è§£ç å™¨ - åœ¨FPNå¢å¼ºç‰¹å¾åŸºç¡€ä¸Šè¿›è¡Œæ™ºèƒ½èåˆè§£ç 
        # # FRDSPé›†æˆäº†fusion_convçš„åˆ†ç»„èåˆå’ŒRDSPçš„å¤šå°ºåº¦ä¸Šä¸‹æ–‡å¢å¼º
        # # ç¬¬ä¸€å±‚è§£ç ï¼šf1(æ·±å±‚) -> f2å°ºåº¦
        # mf1 = self.Up1(enhanced_f1)  # ä¸Šé‡‡æ ·åˆ°f2å°ºåº¦: [B, mid_dim, H2, W2]
        # mf1_concat = torch.cat((mf1, enhanced_f2), dim=1)  # æ‹¼æ¥: [B, 2*mid_dim, H2, W2]
        # mf1 = self.FRDSP1(mf1_concat)  # FRDSPèåˆ: [B, mid_dim, H2, W2]
        #
        # # ç¬¬äºŒå±‚è§£ç ï¼šmf1 -> f3å°ºåº¦
        # mf2 = self.Up2(mf1)  # ä¸Šé‡‡æ ·åˆ°f3å°ºåº¦: [B, mid_dim, H3, W3]
        # mf2_concat = torch.cat((mf2, enhanced_f3), dim=1)  # æ‹¼æ¥: [B, 2*mid_dim, H3, W3]
        # mf2 = self.FRDSP2(mf2_concat)  # FRDSPèåˆ: [B, mid_dim, H3, W3]
        #
        # # ç¬¬ä¸‰å±‚è§£ç ï¼šmf2 -> f4å°ºåº¦
        # mf3 = self.Up3(mf2)  # ä¸Šé‡‡æ ·åˆ°f4å°ºåº¦: [B, mid_dim, H4, W4]
        # mf3_concat = torch.cat((mf3, enhanced_f4), dim=1)  # æ‹¼æ¥: [B, 2*mid_dim, H4, W4]
        # mf3 = self.FRDSP3(mf3_concat)  # FRDSPèåˆ: [B, mid_dim, H4, W4]
        #
        # # ç¬¬å››å±‚è§£ç ï¼šmf3 -> f5å°ºåº¦
        # mf4 = self.Up4(mf3)  # ä¸Šé‡‡æ ·åˆ°f5å°ºåº¦: [B, mid_dim, H5, W5]
        # mf4_concat = torch.cat((mf4, enhanced_f5), dim=1)  # æ‹¼æ¥: [B, 2*mid_dim, H5, W5]
        # mf4 = self.FRDSP4(mf4_concat)  # FRDSPèåˆ: [B, mid_dim, H5, W5]
        #
        # out4 = self.predictor(mf4)

        target_size = f5.shape[-2:]  # ä½¿ç”¨æœ€æµ…å±‚çš„ç©ºé—´å°ºå¯¸ä½œä¸ºç›®æ ‡å°ºå¯¸

        mf1_5 = F.interpolate(f1, size=target_size, mode='bilinear', align_corners=False)
        mf2_5 = F.interpolate(f2, size=target_size, mode='bilinear', align_corners=False)
        mf3_5 = F.interpolate(f3, size=target_size, mode='bilinear', align_corners=False)
        mf4_5 = F.interpolate(f4, size=target_size, mode='bilinear', align_corners=False)

        mf5 = mf1_5 + mf2_5 + mf3_5 + mf4_5 + f5
        mf5 = self.predictor(mf5)
        return mf5

        # return out4


class FRDSP(nn.Module):

    def __init__(self, in_channels, out_channels):
        super(FRDSP, self).__init__()
        assert in_channels == 2 * out_channels, f"æœŸæœ›è¾“å…¥é€šé“æ•°ä¸ºè¾“å‡ºçš„2å€ï¼Œå¾—åˆ° {in_channels} vs {out_channels}"

        self.out_channels = out_channels

        # 1. æºå†…ç‰¹å¾æå–ï¼ˆåŸºäºfusion_convçš„åˆ†ç»„å·ç§¯æ€æƒ³ï¼‰
        # å¯¹è§£ç å™¨ä¸Šé‡‡æ ·ç‰¹å¾å’Œç¼–ç å™¨è·³è·ƒç‰¹å¾åˆ†åˆ«è¿›è¡Œé€æºæå–
        groups = 2  # å›ºå®š2ç»„ï¼š[decoder_features, encoder_skip_features]
        self.source_extraction = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1, groups=groups, bias=True),
            nn.GELU(),
            nn.BatchNorm2d(in_channels)
        )

        # 2. å€’ç½®ç“¶é¢ˆè·¨æºèåˆï¼ˆåŸºäºfusion_convçš„é€ç‚¹å·ç§¯æ€æƒ³ï¼‰
        expansion_factor = 4
        mid_channels = out_channels * expansion_factor

        self.cross_fusion = nn.Sequential(
            # Expand - æ‰©å¼ é€šé“ä»¥å¢å¼ºè¡¨è¾¾èƒ½åŠ›
            nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False),
            nn.GELU(),
            nn.BatchNorm2d(mid_channels),
            # Project - å‹ç¼©å›ç›®æ ‡é€šé“æ•°
            nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=False),
            nn.GELU(),
            nn.BatchNorm2d(out_channels)
        )

        # 3. åŸå§‹RDSPå¤šå°ºåº¦ä¸Šä¸‹æ–‡å¢å¼ºï¼ˆéµå¾ªä¸æ”¹å˜åŸåˆ™ï¼‰
        self.rdsp_context = RDSP(out_channels, out_channels)

        # 4. æ•´ä½“æ®‹å·®è¿æ¥çš„1x1å¯¹é½å±‚
        self.residual_align = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)

        # 5. å¯é€‰çš„è½»é‡æ³¨æ„åŠ›ï¼ˆé€šé“æ³¨æ„åŠ›ï¼‰
        self.channel_attention = ChannelAttention(out_channels)

        self.final_activation = nn.ReLU(inplace=True)

    def forward(self, x):
        # è¾“å…¥: [batch, 2*out_channels, H, W] = [è§£ç å™¨ç‰¹å¾ + ç¼–ç å™¨è·³è·ƒç‰¹å¾]
        identity = x

        # 1. æºå†…ç‰¹å¾æå– - åˆ†åˆ«å¤„ç†ä¸¤ä¸ªæ¥æºçš„ç‰¹å¾ï¼Œé¿å…è¿‡æ—©æ··åˆ
        x = self.source_extraction(x)  # [B, 2*C, H, W]

        # 2. è·¨æºèåˆ - é«˜æ•ˆå¯†é›†çš„ç‰¹å¾èåˆ
        x = self.cross_fusion(x)  # [B, C, H, W]

        # ä¿å­˜èåˆç»“æœç”¨äºæ•´ä½“æ®‹å·®
        fusion_result = x

        # 3. RDSPå¤šå°ºåº¦ä¸Šä¸‹æ–‡å¢å¼º
        x = self.rdsp_context(x)  # [B, C, H, W]

        # 4. é€šé“æ³¨æ„åŠ›
        x = self.channel_attention(x)

        # 5. æ•´ä½“æ®‹å·®è¿æ¥ï¼šåŸå§‹è¾“å…¥ç»1x1å¯¹é½åä¸RDSPè¾“å‡ºç›¸åŠ 
        residual = self.residual_align(identity)  # [B, 2*C, H, W] -> [B, C, H, W]
        x = x + residual

        # 6. æœ€ç»ˆæ¿€æ´»
        x = self.final_activation(x)

        return x


class ChannelAttention(nn.Module):
    """
    è½»é‡çº§é€šé“æ³¨æ„åŠ›æ¨¡å—
    """

    def __init__(self, channels, reduction=8):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.fc = nn.Sequential(
            nn.Conv2d(channels, channels // reduction, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // reduction, channels, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        out = avg_out + max_out
        return x * self.sigmoid(out)


# ğŸ”¬ æ¶ˆèå®éªŒæ¨¡å— - æ–¹æ¡ˆä¸‰çš„ä¸åŒFRDSPæ›¿æ¢æ–¹æ¡ˆ

class SimpleConvFusion(nn.Module):
    """
    æ–¹æ¡ˆä¸‰-1: æ™®é€šå·ç§¯æ›¿æ¢FRDSP
    å¸¦æœ‰è·³è·ƒè¿æ¥çš„3Ã—3å·ç§¯å±‚ï¼Œç±»ä¼¼äºResBlock
    """
    def __init__(self, in_channels, out_channels):
        super(SimpleConvFusion, self).__init__()
        assert in_channels == 2 * out_channels, f"æœŸæœ›è¾“å…¥é€šé“æ•°ä¸ºè¾“å‡ºçš„2å€ï¼Œå¾—åˆ° {in_channels} vs {out_channels}"
        
        # ç®€å•çš„3x3å·ç§¯èåˆ + æ®‹å·®è¿æ¥
        self.conv_fusion = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(out_channels)
        )
        
        # æ®‹å·®è¿æ¥çš„1x1å¯¹é½å±‚
        self.residual_align = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        
        self.final_activation = nn.ReLU(inplace=True)
        
    def forward(self, x):
        # ç®€å•çš„å·ç§¯èåˆ
        conv_out = self.conv_fusion(x)
        
        # æ®‹å·®è¿æ¥
        residual = self.residual_align(x)
        
        # è¾“å‡º
        out = self.final_activation(conv_out + residual)
        return out

class up_conv(nn.Module):
    def __init__(self, ch_in, ch_out):
        super(up_conv, self).__init__()
        self.up = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='bilinear'),
            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),
            nn.BatchNorm2d(ch_out),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        x = self.up(x)
        return x


class PvtV2B2_CGENet(_CGENet_Base):
    def __init__(
            self,
            pretrained=True,
            num_frames=1,
            input_norm=True,
            mid_dim=64,
            siu_groups=4,
            hmu_groups=6,
            use_checkpoint=False,
            use_structure_loss=True,
            loss_weights=None,  # ğŸ”§ æŸå¤±æƒé‡é…ç½®
    ):
        super().__init__()
        self.set_backbone(pretrained=pretrained, use_checkpoint=use_checkpoint)

        # ğŸ”§ æŸå¤±å‡½æ•°é…ç½®
        self.use_structure_loss = use_structure_loss
        if loss_weights is None:
            # é»˜è®¤æƒé‡é…ç½®ï¼ˆåŸºäºè®­ç»ƒæ—¥å¿—åˆ†æï¼‰
            self.loss_weights = {
                'bound': 0.25,
                'structure': 1.0,
                'bce': 1.0,  # BCEæŸå¤±æƒé‡ï¼ˆå½“ä¸ä½¿ç”¨ç»“æ„åŒ–æŸå¤±æ—¶ï¼‰
                'iou': 1.0,  # IOUæŸå¤±æƒé‡ï¼ˆå½“ä¸ä½¿ç”¨ç»“æ„åŒ–æŸå¤±æ—¶ï¼‰
            }
        else:
            self.loss_weights = loss_weights

        self.embed_dims = self.encoder.embed_dims
        # self.tra_5 = EFF_ImprovedSA(self.embed_dims[3], out_dim=mid_dim, is_bottom=True)
        self.tra_5 = SimpleASPP(self.embed_dims[3], out_dim=mid_dim)
        # self.tra_5 = ConvBNReLU(self.embed_dims[3], mid_dim, 3, 1, 1)
        self.tra_4 = ConvBNReLU(self.embed_dims[2], mid_dim, 3, 1, 1)
        self.tra_3 = ConvBNReLU(self.embed_dims[1], mid_dim, 3, 1, 1)
        self.tra_2 = ConvBNReLU(self.embed_dims[0], mid_dim, 3, 1, 1)
        self.tra_1 = nn.Sequential(
            nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False), ConvBNReLU(64, mid_dim, 3, 1, 1)
        )

        self.normalizer = PixelNormalizer() if input_norm else nn.Identity()
        self.predictor = nn.Sequential(
            nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False),
            ConvBNReLU(64, 32, 3, 1, 1),
            nn.Conv2d(32, 1, 1),
        )

        # self.predictor_64 = nn.Sequential(
        #     # ConvBNReLU(128, 64, 3, 1, 1),
        #     ConvBNReLU(64, 32, 3, 1, 1),
        #     nn.Conv2d(32, 1, 1),
        # )

        self.dwc1 = conv3x3_bn_relu(512, 320, k=1, s=1, p=0)
        self.dwc2 = conv3x3_bn_relu(320, 128, k=1, s=1, p=0)
        self.dwc3 = conv3x3_bn_relu(128, 64, k=1, s=1, p=0)
        self.dwcon_2 = conv3x3_bn_relu(320, 320)
        self.dwcon_3 = conv3x3_bn_relu(128, 128)
        self.dwcon_4 = conv3x3_bn_relu(64, 64)

        # self.siu_5 = SimpleConcatFusion(in_channels=mid_dim, out_channels=mid_dim)
        # self.siu_4 = SimpleConcatFusion(in_channels=mid_dim, out_channels=mid_dim)
        # self.siu_3 = SimpleConcatFusion(in_channels=mid_dim, out_channels=mid_dim)
        # self.siu_2 = SimpleConcatFusion(in_channels=mid_dim, out_channels=mid_dim)

        self.siu_5 = MultiHeadMyNet(mid_dim, num_heads=4)
        self.siu_4 = MultiHeadMyNet(mid_dim, num_heads=4)
        self.siu_3 = MultiHeadMyNet(mid_dim, num_heads=4)
        self.siu_2 = MultiHeadMyNet(mid_dim, num_heads=4)

        # Decoder
        self.Up1 = up_conv(ch_in=mid_dim, ch_out=mid_dim)
        self.Up2 = up_conv(ch_in=mid_dim, ch_out=mid_dim)
        self.Up3 = up_conv(ch_in=mid_dim, ch_out=mid_dim)


        # è¾“å…¥é€šé“æ•°ä¸º mid_dim*2 (ä¸Šé‡‡æ ·ç‰¹å¾ + è·³è·ƒè¿æ¥ç‰¹å¾)
        # è¾“å‡ºé€šé“æ•°ä¸º mid_dim
        self.FRDSP1 = FRDSP(mid_dim * 2, mid_dim)  # ç¬¬ä¸€å±‚è§£ç èåˆ
        self.FRDSP2 = FRDSP(mid_dim * 2, mid_dim)  # ç¬¬äºŒå±‚è§£ç èåˆ
        self.FRDSP3 = FRDSP(mid_dim * 2, mid_dim)  # ç¬¬ä¸‰å±‚è§£ç èåˆ

        # ğŸ”§ ä¼˜åŒ–çš„FPNç‰¹å¾ä¼ é€’é—¨æ§æœºåˆ¶
        # ä¸ºè·¨å°ºåº¦ç‰¹å¾ä¼ é€’æ·»åŠ å¯å­¦ä¹ æƒé‡ï¼Œé¿å…è¿‡å¼ºæ³¨å…¥
        self.fpn_gate_1_2 = nn.Parameter(torch.ones(1))  # f1 -> f2 çš„æƒé‡é—¨æ§
        self.fpn_gate_1_3 = nn.Parameter(torch.ones(1))  # f1 -> f3 çš„æƒé‡é—¨æ§
        self.fpn_gate_1_4 = nn.Parameter(torch.ones(1))  # f1 -> f4 çš„æƒé‡é—¨æ§
        self.fpn_gate_2_3 = nn.Parameter(torch.ones(1))  # f2 -> f3 çš„æƒé‡é—¨æ§
        self.fpn_gate_2_4 = nn.Parameter(torch.ones(1))  # f2 -> f4 çš„æƒé‡é—¨æ§
        self.fpn_gate_3_4 = nn.Parameter(torch.ones(1))  # f3 -> f4 çš„æƒé‡é—¨æ§

        # FPNç‰¹å¾å¯¹é½å±‚ï¼ˆå¯é€‰ï¼Œå½“éœ€è¦æ›´å¥½çš„ç‰¹å¾å¯¹é½æ—¶ä½¿ç”¨ï¼‰
        self.fpn_align_1_2 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)
        self.fpn_align_1_3 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)
        self.fpn_align_1_4 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)
        self.fpn_align_2_3 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)
        self.fpn_align_2_4 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)
        self.fpn_align_3_4 = nn.Conv2d(mid_dim, mid_dim, kernel_size=1, bias=False)

    def set_backbone(self, pretrained: bool, use_checkpoint: bool):
        self.encoder = pvt_v2_eff_b2(pretrained=pretrained, use_checkpoint=use_checkpoint)

    def normalize_encoder(self, x):
        x = self.normalizer(x)
        features = self.encoder(x)
        c2 = features["reduction_2"]  # l:[8, 64, 144, 144] m:[8, 64, 96, 96]  s:[8, 64, 120, 120]
        c3 = features["reduction_3"]  # l:[8, 128, 72, 72]  m:[8, 128, 48, 48] s:[8, 128, 60, 60]
        c4 = features["reduction_4"]  # l:[8, 320, 36, 36]  m:[8, 320, 24, 24] s:[8, 320, 30, 30]
        c5 = features["reduction_5"]  # l:[8, 512, 18, 18]  m:[8, 512, 12, 12] s:[8, 512, 15, 15]
        return c2, c3, c4, c5

    def body(self, data):
        l_trans_feats = self.normalize_encoder(data["image_l"])  # [8, 3, 576, 576]
        m_trans_feats = self.normalize_encoder(data["image_m"])  # [8, 3, 480, 480]
        s_trans_feats = self.normalize_encoder(data["image_s"])  # [8, 3, 384, 384]

        l1, m1, s1 = self.tra_5(l_trans_feats[3]), self.tra_5(m_trans_feats[3]), self.tra_5(s_trans_feats[3])
        f1 = self.siu_5(l=l1, m=m1, s=s1)  # [8, 64, 12, 12] (mid_dim)
        l2, m2, s2 = self.tra_4(l_trans_feats[2]), self.tra_4(m_trans_feats[2]), self.tra_4(s_trans_feats[2])
        f2 = self.siu_4(l=l2, m=m2, s=s2)  # [8, 64, 24, 24] (mid_dim)
        l3, m3, s3 = self.tra_3(l_trans_feats[1]), self.tra_3(m_trans_feats[1]), self.tra_3(s_trans_feats[1])
        f3 = self.siu_3(l=l3, m=m3, s=s3)  # [8, 64, 48, 48] (mid_dim)
        l4, m4, s4 = self.tra_2(l_trans_feats[0]), self.tra_2(m_trans_feats[0]), self.tra_2(s_trans_feats[0])
        f4 = self.siu_2(l=l4, m=m4, s=s4)  # [8, 64, 96, 96] (mid_dim)
        # l1, s1 = self.tra_5(l_trans_feats[3]), self.tra_5(s_trans_feats[3])
        # f1 = self.siu_5(l=l1, s=s1)  # [8, 64, 12, 12] (mid_dim)
        # l2, s2 = self.tra_4(l_trans_feats[2]), self.tra_4(s_trans_feats[2])
        # f2 = self.siu_4(l=l2, s=s2)  # [8, 64, 24, 24] (mid_dim)
        # l3, s3 = self.tra_3(l_trans_feats[1]), self.tra_3(s_trans_feats[1])
        # f3 = self.siu_3(l=l3,  s=s3)  # [8, 64, 48, 48] (mid_dim)
        # l4, s4 = self.tra_2(l_trans_feats[0]), self.tra_2(s_trans_feats[0])
        # f4 = self.siu_2(l=l4, s=s4)  # [8, 64, 96, 96] (mid_dim)

        # ğŸ”§ ä¼˜åŒ–çš„FPNå¤šå°ºåº¦ç‰¹å¾èåˆ - åŠ å…¥é—¨æ§æœºåˆ¶ï¼Œæ›´é€‚åˆä¼ªè£…ç›®æ ‡æ£€æµ‹
        # ä¼ªè£…ç›®æ ‡éœ€è¦å¤šå°ºåº¦ä¿¡æ¯è¿›è¡Œå‡†ç¡®æ£€æµ‹ï¼Œä½†éœ€è¦é¿å…è¿‡å¼ºçš„ç‰¹å¾æ³¨å…¥å¯¼è‡´ä¿¡æ¯å†²çª

        # å¤šå°ºåº¦æ’å€¼ï¼šå°†æ·±å±‚ç‰¹å¾ä¼ æ’­åˆ°æµ…å±‚ï¼Œå¹¶é€šè¿‡å¯¹é½å±‚è¿›è¡Œç‰¹å¾å¯¹é½
        mf1_2 = F.interpolate(f1, size=f2.shape[-2:], mode='bilinear', align_corners=False)
        mf1_2 = self.fpn_align_1_2(mf1_2)  # ç‰¹å¾å¯¹é½

        mf1_3 = F.interpolate(f1, size=f3.shape[-2:], mode='bilinear', align_corners=False)
        mf1_3 = self.fpn_align_1_3(mf1_3)  # ç‰¹å¾å¯¹é½

        mf1_4 = F.interpolate(f1, size=f4.shape[-2:], mode='bilinear', align_corners=False)
        mf1_4 = self.fpn_align_1_4(mf1_4)  # ç‰¹å¾å¯¹é½

        mf2_3 = F.interpolate(f2, size=f3.shape[-2:], mode='bilinear', align_corners=False)
        mf2_3 = self.fpn_align_2_3(mf2_3)  # ç‰¹å¾å¯¹é½

        mf2_4 = F.interpolate(f2, size=f4.shape[-2:], mode='bilinear', align_corners=False)
        mf2_4 = self.fpn_align_2_4(mf2_4)  # ç‰¹å¾å¯¹é½

        mf3_4 = F.interpolate(f3, size=f4.shape[-2:], mode='bilinear', align_corners=False)
        mf3_4 = self.fpn_align_3_4(mf3_4)  # ç‰¹å¾å¯¹é½

        enhanced_f1 = f1  # æœ€æ·±å±‚ä¿æŒåŸæ ·
        enhanced_f2 = f2 + self.fpn_gate_1_2 * mf1_2  # é—¨æ§èåˆæ¥è‡ªf1çš„è¯­ä¹‰ä¿¡æ¯
        enhanced_f3 = f3 + self.fpn_gate_1_3 * mf1_3 + self.fpn_gate_2_3 * mf2_3  # é—¨æ§èåˆå¤šå°ºåº¦ä¿¡æ¯
        enhanced_f4 = f4 + self.fpn_gate_1_4 * mf1_4 + self.fpn_gate_2_4 * mf2_4 + self.fpn_gate_3_4 * mf3_4  # é—¨æ§èåˆæ‰€æœ‰å°ºåº¦ä¿¡æ¯


        # ç¬¬ä¸€å±‚è§£ç ï¼šf1(æ·±å±‚) -> f2å°ºåº¦
        mf1 = self.Up1(enhanced_f1)  # ä¸Šé‡‡æ ·åˆ°f2å°ºåº¦: [B, mid_dim, H2, W2]
        mf1_concat = torch.cat((mf1, enhanced_f2), dim=1)  # æ‹¼æ¥: [B, 2*mid_dim, H2, W2]
        mf1 = self.FRDSP1(mf1_concat)  # FRDSPèåˆ: [B, mid_dim, H2, W2]

        # ç¬¬äºŒå±‚è§£ç ï¼šmf1 -> f3å°ºåº¦
        mf2 = self.Up2(mf1)  # ä¸Šé‡‡æ ·åˆ°f3å°ºåº¦: [B, mid_dim, H3, W3]
        mf2_concat = torch.cat((mf2, enhanced_f3), dim=1)  # æ‹¼æ¥: [B, 2*mid_dim, H3, W3]
        mf2 = self.FRDSP2(mf2_concat)  # FRDSPèåˆ: [B, mid_dim, H3, W3]

        # ç¬¬ä¸‰å±‚è§£ç ï¼šmf2 -> f4å°ºåº¦
        mf3 = self.Up3(mf2)  # ä¸Šé‡‡æ ·åˆ°f4å°ºåº¦: [B, mid_dim, H4, W4]
        mf3_concat = torch.cat((mf3, enhanced_f4), dim=1)  # æ‹¼æ¥: [B, 2*mid_dim, H4, W4]
        mf3 = self.FRDSP3(mf3_concat)  # FRDSPèåˆ: [B, mid_dim, H4, W4]

        out4 = self.predictor(mf3)

        out4 = F.interpolate(out4, size=data["image_s"].size()[2:], mode='bilinear', align_corners=True)

        return out4
        # out = self.predictor(enhanced_f4)
        # out = F.interpolate(out, size=data["image_s"].size()[2:], mode='bilinear', align_corners=True)
        # return out

class PvtV2B1_CGENet(PvtV2B2_CGENet):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def set_backbone(self, pretrained: bool, use_checkpoint: bool):
        self.encoder = pvt_v2_eff_b1(pretrained=pretrained, use_checkpoint=use_checkpoint)

class PvtV2B3_CGENet(PvtV2B2_CGENet):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def set_backbone(self, pretrained: bool, use_checkpoint: bool):
        self.encoder = pvt_v2_eff_b3(pretrained=pretrained, use_checkpoint=use_checkpoint)


class PvtV2B4_CGENet(PvtV2B2_CGENet):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def set_backbone(self, pretrained: bool, use_checkpoint: bool):
        self.encoder = pvt_v2_eff_b4(pretrained=pretrained, use_checkpoint=use_checkpoint)


class PvtV2B5_CGENet(PvtV2B2_CGENet):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def set_backbone(self, pretrained: bool, use_checkpoint: bool):
        self.encoder = pvt_v2_eff_b5(pretrained=pretrained, use_checkpoint=use_checkpoint)


class videoPvtV2B5_CGENet(PvtV2B5_CGENet):
    def get_grouped_params(self):
        param_groups = {"pretrained": [], "fixed": [], "retrained": []}
        for name, param in self.named_parameters():
            if name.startswith("encoder.patch_embed1."):
                param.requires_grad = False
                param_groups["fixed"].append(param)
            elif name.startswith("encoder."):
                param_groups["pretrained"].append(param)
            else:
                if "temperal_proj" in name:
                    param_groups["retrained"].append(param)
                else:
                    param_groups["pretrained"].append(param)

        LOGGER.info(
            f"Parameter Groups:{{"
            f"Pretrained: {len(param_groups['pretrained'])}, "
            f"Fixed: {len(param_groups['fixed'])}, "
            f"ReTrained: {len(param_groups['retrained'])}}}"
        )
        return param_groups


# class EffB1_CGENet(_CGENet_Base):
#     def __init__(self, pretrained, num_frames=1, input_norm=True, mid_dim=64, siu_groups=4, hmu_groups=6, **kwargs):
#         super().__init__()
#         self.set_backbone(pretrained)
#
#         self.tra_5 = SimpleASPP(self.embed_dims[4], out_dim=mid_dim)
#         self.siu_5 = MHSIU(mid_dim, siu_groups)
#         self.hmu_5 = RGPU(mid_dim, hmu_groups, num_frames=num_frames)
#
#         self.tra_4 = ConvBNReLU(self.embed_dims[3], mid_dim, 3, 1, 1)
#         self.siu_4 = MHSIU(mid_dim, siu_groups)
#         self.hmu_4 = RGPU(mid_dim, hmu_groups, num_frames=num_frames)
#
#         self.tra_3 = ConvBNReLU(self.embed_dims[2], mid_dim, 3, 1, 1)
#         self.siu_3 = MHSIU(mid_dim, siu_groups)
#         self.hmu_3 = RGPU(mid_dim, hmu_groups, num_frames=num_frames)
#
#         self.tra_2 = ConvBNReLU(self.embed_dims[1], mid_dim, 3, 1, 1)
#         self.siu_2 = MHSIU(mid_dim, siu_groups)
#         self.hmu_2 = RGPU(mid_dim, hmu_groups, num_frames=num_frames)
#
#         self.tra_1 = ConvBNReLU(self.embed_dims[0], mid_dim, 3, 1, 1)
#         self.siu_1 = MHSIU(mid_dim, siu_groups)
#         self.hmu_1 = RGPU(mid_dim, hmu_groups, num_frames=num_frames)
#
#         self.normalizer = PixelNormalizer() if input_norm else nn.Identity()
#         self.predictor = nn.Sequential(
#             nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False),
#             ConvBNReLU(64, 32, 3, 1, 1),
#             nn.Conv2d(32, 1, 1),
#         )
#
#     def set_backbone(self, pretrained):
#         self.encoder = EfficientNet.from_pretrained("efficientnet-b1", pretrained=pretrained)
#         self.embed_dims = [16, 24, 40, 112, 320]
#
#     def normalize_encoder(self, x):
#         x = self.normalizer(x)
#         features = self.encoder.extract_endpoints(x)
#         c1 = features["reduction_1"]
#         c2 = features["reduction_2"]
#         c3 = features["reduction_3"]
#         c4 = features["reduction_4"]
#         c5 = features["reduction_5"]
#         return c1, c2, c3, c4, c5
#
#     def body(self, data):
#         l_trans_feats = self.normalize_encoder(data["image_l"])
#         m_trans_feats = self.normalize_encoder(data["image_m"])
#         s_trans_feats = self.normalize_encoder(data["image_s"])
#
#         l, m, s = self.tra_5(l_trans_feats[4]), self.tra_5(m_trans_feats[4]), self.tra_5(s_trans_feats[4])
#         lms = self.siu_5(l=l, m=m, s=s)
#         x = self.hmu_5(lms)
#
#         l, m, s = self.tra_4(l_trans_feats[3]), self.tra_4(m_trans_feats[3]), self.tra_4(s_trans_feats[3])
#         lms = self.siu_4(l=l, m=m, s=s)
#         x = self.hmu_4(lms + resize_to(x, tgt_hw=lms.shape[-2:]))
#
#         l, m, s = self.tra_3(l_trans_feats[2]), self.tra_3(m_trans_feats[2]), self.tra_3(s_trans_feats[2])
#         lms = self.siu_3(l=l, m=m, s=s)
#         x = self.hmu_3(lms + resize_to(x, tgt_hw=lms.shape[-2:]))
#
#         l, m, s = self.tra_2(l_trans_feats[1]), self.tra_2(m_trans_feats[1]), self.tra_2(s_trans_feats[1])
#         lms = self.siu_2(l=l, m=m, s=s)
#         x = self.hmu_2(lms + resize_to(x, tgt_hw=lms.shape[-2:]))
#
#         l, m, s = self.tra_1(l_trans_feats[0]), self.tra_1(m_trans_feats[0]), self.tra_1(s_trans_feats[0])
#         lms = self.siu_1(l=l, m=m, s=s)
#         x = self.hmu_1(lms + resize_to(x, tgt_hw=lms.shape[-2:]))
#
#         return self.predictor(x)


# class EffB4_CGENet(EffB1_CGENet):
#     def set_backbone(self, pretrained):
#         self.encoder = EfficientNet.from_pretrained("efficientnet-b4", pretrained=pretrained)
#         self.embed_dims = [24, 32, 56, 160, 448]

if __name__ == "__main__":
    model1 = PvtV2B2_CGENet(
        pretrained=True,
        use_checkpoint=False,
        use_structure_loss=True
    ).cuda()

    # ç¤ºä¾‹2ï¼šä½¿ç”¨åˆ†ç¦»çš„BCE+IOUæŸå¤±
    model2 = PvtV2B2_CGENet(
        pretrained=True,
        use_checkpoint=False,
        use_structure_loss=False  # ä½¿ç”¨åˆ†ç¦»çš„BCE+IOUæŸå¤±
    ).cuda()


    custom_weights = {
        'bound': 0.3,
        'structure': 1.5,  # é™ä½ç»“æ„åŒ–æŸå¤±æƒé‡
        'bce': 1.0,  # BCEæŸå¤±æƒé‡
        'iou': 1.2,  # å¢åŠ IOUæŸå¤±æƒé‡
    }
    model3 = PvtV2B2_CGENet(
        pretrained=True,
        use_checkpoint=False,
        use_structure_loss=False,
        loss_weights=custom_weights
    ).cuda()

    # æµ‹è¯•å‰å‘ä¼ æ’­
    input_tensor = torch.randn(2, 3, 384, 384).cuda()
    output = model1(input_tensor)
    print(f"Output shape: {output.shape}")

    # æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„æŸå¤±è®¡ç®—
    dummy_data = {
        "image_l": torch.randn(2, 3, 576, 576).cuda(),
        "image_m": torch.randn(2, 3, 384, 384).cuda(),
        "image_s": torch.randn(2, 3, 480, 480).cuda(),
        "mask": torch.randint(0, 2, (2, 1, 384, 384)).float().cuda()
    }

    model1.train()
    result = model1(dummy_data, iter_percentage=0.5)
    print(f"Loss: {result['loss'].item():.5f}")
    print(f"Loss breakdown: {result['loss_str']}")
