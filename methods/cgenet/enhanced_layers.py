"""
å¢å¼ºçš„MHSIUæ¨¡å—å®ç°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

from methods.cgenet.layers import DifferenceAwareOps
from .ops import ConvBNReLU


# ===============================================================================
# åŸºç¡€æ³¨æ„åŠ›æ¨¡å—
# ===============================================================================
class MyNet(nn.Module):
    def __init__(self, in_c, num_groups=3, hidden_dim=None, num_frames=1):
        super().__init__()

        # é¢„å¤„ç†å±‚ - ä¿æŒå„å°ºåº¦çš„ç‹¬ç«‹æ€§
        self.conv_l_pre = ConvBNReLU(in_c, in_c, 3, 1, 1)
        self.conv_s_pre = ConvBNReLU(in_c, in_c, 3, 1, 1)
        self.conv_l = ConvBNReLU(in_c, in_c, 3, 1, 1)  # intra-branch
        self.conv_m = ConvBNReLU(in_c, in_c, 3, 1, 1)  # intra-branch
        self.conv_s = ConvBNReLU(in_c, in_c, 3, 1, 1)  # intra-branch

        self.num_groups = num_groups
        hidden_dim = hidden_dim or in_c

        self.gate_genator = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Conv2d(num_groups * hidden_dim, hidden_dim, 1),
            nn.ReLU(True),
            nn.Conv2d(hidden_dim, num_groups * hidden_dim, 1),
            nn.Softmax(dim=1),
        )

        self.interact = nn.ModuleDict()
        self.interact["0"] = ConvBNReLU(hidden_dim, 3 * hidden_dim, 3, 1, 1)
        for group_id in range(1, num_groups - 1):
            self.interact[str(group_id)] = ConvBNReLU(2 * hidden_dim, 3 * hidden_dim, 3, 1, 1)
        self.interact[str(num_groups - 1)] = ConvBNReLU(2 * hidden_dim, 2 * hidden_dim, 3, 1, 1)

        self.fuse = nn.Sequential(
            DifferenceAwareOps(num_frames=num_frames),
            ConvBNReLU(num_groups * hidden_dim, in_c, 3, 1, 1, act_name=None),
        )
        self.final_relu = nn.ReLU(True)

    def forward(self, l, m, s):  # expand_conv(x)[2, 192,12,12]
        tgt_size = s.shape[2:]

        l = self.conv_l_pre(l)
        l = F.adaptive_max_pool2d(l, tgt_size) + F.adaptive_avg_pool2d(l, tgt_size)
        m = self.conv_s_pre(m)
        m = F.adaptive_max_pool2d(m, tgt_size) + F.adaptive_avg_pool2d(m, tgt_size)

        l = self.conv_l(l)
        m = self.conv_m(m)
        s = self.conv_s(s)

        outs = []
        gates = []
        # åˆ›å»ºä¸¤ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºæ”¶é›†å„ç»„çš„è¾“å‡ºç‰¹å¾å’Œé—¨æ§ç‰¹å¾ã€‚

        group_id = 0
        branch_out = self.interact[str(group_id)](l)  # å¯¹1.5å€æ”¾å¤§å›¾åƒ-læ‰©å±•åˆ°é«˜ç»´åº¦ [b, 64, h, w] -> [b, 64*3, h, w]
        curr_out, curr_fork, curr_gate = branch_out.chunk(3, dim=1)  # åˆ’åˆ†æˆ3ä¸ªåˆ†æ”¯
        # curr_out:å½“å‰ç»„çš„è¾“å‡ºç‰¹å¾â€”â€”g3ï¼Œ curr_fork:ä¼ é€’ç‰¹å¾â€”â€”g1ï¼Œ curr_gate:é—¨æ§ç‰¹å¾â€”â€”g2
        outs.append(curr_out)
        gates.append(curr_gate)

        group_id = 1
        curr_m = torch.cat([m, curr_fork], dim=1)  # åˆ›å»ºä¸€ä¸ªæ–°å¼ é‡ï¼Œå°†måŸå§‹ç‰¹å¾å’Œlä¼ é€’ç‰¹å¾è¿›è¡Œæ‹¼æ¥
        branch_out = self.interact[str(group_id)](curr_m)  # å¯¹æ‹¼æ¥å›¾åƒæ‰©å±•åˆ°é«˜ç»´åº¦ [b, 64*2, h, w] -> [b, 64*3, h, w]
        curr_out, curr_fork, curr_gate = branch_out.chunk(3, dim=1)  # åˆ’åˆ†æˆ3ä¸ªåˆ†æ”¯
        outs.append(curr_out)
        gates.append(curr_gate)

        group_id = 2
        curr_x = torch.cat([s, curr_fork], dim=1)  # åˆ›å»ºä¸€ä¸ªæ–°å¼ é‡ï¼Œå°†såŸå§‹ç‰¹å¾å’Œmä¼ é€’ç‰¹å¾è¿›è¡Œæ‹¼æ¥
        branch_out = self.interact[str(group_id)](curr_x)  # å¯¹æ‹¼æ¥å›¾åƒå·ç§¯ [b, 64*2, h, w] -> [b, 64*2, h, w]
        curr_out, curr_gate = branch_out.chunk(2, dim=1)
        outs.append(curr_out)
        gates.append(curr_gate)

        out = torch.cat(outs, dim=1)
        gate = self.gate_genator(torch.cat(gates, dim=1))
        out = self.fuse(out * gate)
        return self.final_relu(out + s)


class MultiHeadMyNet(nn.Module):
    def __init__(self, in_c, num_groups=3, num_heads=4, hidden_dim=None, num_frames=1,
                 gate_type='softmax', fusion_type='conv', norm_type='bn'):
        """
        MultiHeadMyNet æ¨¡å—çš„æ¶ˆèå®éªŒç‰ˆæœ¬
        
        Args:
            gate_type (str): é—¨æ§æ–¹å¼ 'softmax'|'sigmoid'|'none'
            fusion_type (str): å¤´é—´èåˆæ–¹å¼ 'conv'|'weighted_sum'
            norm_type (str): å½’ä¸€åŒ–ç±»å‹ 'bn'|'gn'
        """
        super().__init__()

        self.gate_type = gate_type
        self.fusion_type = fusion_type
        self.norm_type = norm_type

        # æ ¹æ®å½’ä¸€åŒ–ç±»å‹é€‰æ‹©ConvNormActå‡½æ•°
        if norm_type == 'bn':
            ConvNormAct = lambda in_c, out_c, k, s=1, p=None, act_name='relu': ConvBNReLU(in_c, out_c, k, s, p if p is not None else k//2, act_name=act_name)
        elif norm_type == 'gn':
            ConvNormAct = lambda in_c, out_c, k, s=1, p=None, act_name='silu': ConvGNSiLU(in_c, out_c, k, s, p if p is not None else k//2, act_name=act_name)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å½’ä¸€åŒ–ç±»å‹: {norm_type}")

        # ä¿æŒåŸå§‹çš„é¢„å¤„ç†å±‚
        self.conv_l_pre = ConvNormAct(in_c, in_c, 3, 1, 1)
        self.conv_m_pre = ConvNormAct(in_c, in_c, 3, 1, 1)
        self.conv_s_pre = ConvNormAct(in_c, in_c, 3, 1, 1)

        self.conv_l = ConvNormAct(in_c, in_c, 3, 1, 1)  # intra-branch
        self.conv_m = ConvNormAct(in_c, in_c, 3, 1, 1)  # intra-branch
        self.conv_s = ConvNormAct(in_c, in_c, 3, 1, 1)  # intra-branch

        self.num_groups = num_groups
        self.num_heads = num_heads
        hidden_dim = hidden_dim or in_c

        # å½“ num_heads == 0 æ—¶ï¼Œèµ°å•å¤´ MyNet ç›´é€šè·¯å¾„ï¼›>0 æ—¶å¯ç”¨å¤šå¤´
        self.use_multihead = (self.num_heads is not None) and (self.num_heads > 0)
        if not self.use_multihead:
            self.single_head = MyNet(in_c=in_c, num_groups=num_groups, hidden_dim=hidden_dim, num_frames=num_frames)
        else:
            # ğŸ”§ æ¶ˆè1: é—¨æ§æ–¹å¼æ¶ˆè
            self.gate_generators = nn.ModuleList()
            for _ in range(num_heads):
                if gate_type == 'none':
                    # æ— é—¨æ§ï¼šç›´æ¥è¿”å›å…¨1æƒé‡
                    self.gate_generators.append(nn.Identity())
                else:
                    # æœ‰é—¨æ§ï¼šæ ¹æ®ç±»å‹é€‰æ‹©æ¿€æ´»å‡½æ•°
                    gate_activation = nn.Softmax(dim=1) if gate_type == 'softmax' else nn.Sigmoid()
                    self.gate_generators.append(nn.Sequential(
                        nn.AdaptiveAvgPool2d((1, 1)),
                        nn.Conv2d(num_groups * hidden_dim, hidden_dim, 1),
                        nn.ReLU(True),
                        nn.Conv2d(hidden_dim, num_groups * hidden_dim, 1),
                        gate_activation,
                    ))

            # å¤šå¤´äº¤äº’æ¨¡å— - ä½¿ç”¨é€‰å®šçš„å½’ä¸€åŒ–æ–¹å¼
            self.interact_heads = nn.ModuleList()
            for head in range(num_heads):
                head_interact = nn.ModuleDict()
                head_interact["0"] = ConvNormAct(hidden_dim, 3 * hidden_dim, 3, 1, 1)
                for group_id in range(1, num_groups - 1):
                    head_interact[str(group_id)] = ConvNormAct(2 * hidden_dim, 3 * hidden_dim, 3, 1, 1)
                head_interact[str(num_groups - 1)] = ConvNormAct(2 * hidden_dim, 2 * hidden_dim, 3, 1, 1)
                self.interact_heads.append(head_interact)

            # ğŸ”§ æ¶ˆè2: å¤´é—´èåˆæ–¹å¼æ¶ˆè
            if fusion_type == 'conv':
                # Conv1x1èåˆ
                self.head_fusion = ConvNormAct(num_heads * num_groups * hidden_dim, num_groups * hidden_dim, 1)
            elif fusion_type == 'weighted_sum':
                # å¯å­¦ä¹ æ ‡é‡åŠ æƒæ±‚å’Œ
                self.head_weights = nn.Parameter(torch.ones(num_heads) / num_heads)  # åˆå§‹åŒ–ä¸ºå‡ç­‰æƒé‡
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„èåˆç±»å‹: {fusion_type}")

            # ä¿æŒåŸå§‹çš„èåˆå±‚ - ä½¿ç”¨é€‰å®šçš„å½’ä¸€åŒ–æ–¹å¼
            self.fuse = nn.Sequential(
                DifferenceAwareOps(num_frames=num_frames),
                ConvNormAct(num_groups * hidden_dim, in_c, 3, 1, 1, act_name=None),
            )
            
            # ğŸ”§ æ¶ˆè3: æœ€ç»ˆæ¿€æ´»å‡½æ•°æ ¹æ®å½’ä¸€åŒ–ç±»å‹é€‰æ‹©
            if norm_type == 'bn':
                self.final_activation = nn.ReLU(True)
            elif norm_type == 'gn':
                self.final_activation = nn.SiLU(True)


    def forward(self, l, m, s):
        if not getattr(self, 'use_multihead', True):
            return self.single_head(l, m, s)

        # ä¿æŒä»¥så°ºåº¦ä¸ºåŸºå‡†
        tgt_size = s.shape[2:]

        # é¢„å¤„ç† - ä¸åŸå§‹MyNetå®Œå…¨ä¸€è‡´
        l = self.conv_l_pre(l)
        l = F.adaptive_max_pool2d(l, tgt_size) + F.adaptive_avg_pool2d(l, tgt_size)

        m = self.conv_m_pre(m)
        m = F.adaptive_max_pool2d(m, tgt_size) + F.adaptive_avg_pool2d(m, tgt_size)

        l = self.conv_l(l)
        m = self.conv_m(m)
        s = self.conv_s(s)

        # å¤šå¤´å¹¶è¡Œå¤„ç†
        all_head_outs = []
        # all_head_gates = []

        for head_idx in range(self.num_heads):
            # æ¯ä¸ªå¤´ç‹¬ç«‹æ‰§è¡ŒåŸå§‹MyNetçš„é€»è¾‘
            outs = []
            gates = []

            # Group 0: å¤„ç†lå°ºåº¦
            group_id = 0
            branch_out = self.interact_heads[head_idx][str(group_id)](l)
            curr_out, curr_fork, curr_gate = branch_out.chunk(3, dim=1)
            outs.append(curr_out)
            gates.append(curr_gate)

            # Group 1: å¤„ç†må°ºåº¦ + lçš„fork
            group_id = 1
            curr_m = torch.cat([m, curr_fork], dim=1)
            branch_out = self.interact_heads[head_idx][str(group_id)](curr_m)
            curr_out, curr_fork, curr_gate = branch_out.chunk(3, dim=1)
            outs.append(curr_out)
            gates.append(curr_gate)

            # Group 2: å¤„ç†så°ºåº¦ + mçš„fork
            group_id = 2
            curr_x = torch.cat([s, curr_fork], dim=1)
            branch_out = self.interact_heads[head_idx][str(group_id)](curr_x)
            curr_out, curr_gate = branch_out.chunk(2, dim=1)
            outs.append(curr_out)
            gates.append(curr_gate)

            # å½“å‰å¤´çš„ç‰¹å¾æ‹¼æ¥
            head_out = torch.cat(outs, dim=1)  # [B, num_groups * hidden_dim, H, W]
            
            # ğŸ”§ æ¶ˆè1: é—¨æ§æ–¹å¼å¤„ç†
            if self.gate_type == 'none':
                # æ— é—¨æ§ï¼šç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾
                gated_head_out = head_out
            else:
                # æœ‰é—¨æ§ï¼šåº”ç”¨é—¨æ§æƒé‡
                head_gate = self.gate_generators[head_idx](torch.cat(gates, dim=1))
                gated_head_out = head_out * head_gate

            all_head_outs.append(gated_head_out)

        # ğŸ”§ æ¶ˆè2: å¤´é—´èåˆæ–¹å¼å¤„ç†
        if self.fusion_type == 'conv':
            # Conv1x1èåˆ
            multi_head_out = torch.cat(all_head_outs, dim=1)  # [B, num_heads * num_groups * hidden_dim, H, W]
            fused_out = self.head_fusion(multi_head_out)  # [B, num_groups * hidden_dim, H, W]
        elif self.fusion_type == 'weighted_sum':
            # å¯å­¦ä¹ æ ‡é‡åŠ æƒæ±‚å’Œ
            weights = F.softmax(self.head_weights, dim=0)  # ç¡®ä¿æƒé‡å’Œä¸º1
            fused_out = sum(w * head_out for w, head_out in zip(weights, all_head_outs))

        # æœ€ç»ˆèåˆå’Œæ®‹å·®è¿æ¥
        final_out = self.fuse(fused_out)  # [B, in_c, H, W]
        return self.final_activation(final_out + s)


# è¾…åŠ©å‡½æ•°ï¼šGroupNorm + SiLU ç‰ˆæœ¬çš„ConvNormAct
class ConvGNSiLU(nn.Module):
    """GroupNorm + SiLU ç‰ˆæœ¬çš„å·ç§¯å—"""

    def __init__(self, in_c, out_c, kernel_size, stride=1, padding=0, groups=1, act_name='silu'):
        super().__init__()
        self.conv = nn.Conv2d(in_c, out_c, kernel_size, stride, padding, groups=groups, bias=False)

        # GroupNorm: ä½¿ç”¨16ä¸ªç»„ï¼ˆæˆ–é€šé“æ•°çš„1/8ï¼Œæœ€å°‘1ç»„ï¼‰
        num_groups = min(16, max(1, out_c // 8))
        self.norm = nn.GroupNorm(num_groups, out_c)

        if act_name == 'silu':
            self.act = nn.SiLU(True)
        elif act_name == 'relu':
            self.act = nn.ReLU(True)
        elif act_name is None:
            self.act = nn.Identity()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°: {act_name}")

    def forward(self, x):
        x = self.conv(x)
        x = self.norm(x)
        x = self.act(x)
        return x

class scSE(nn.Module):

    def __init__(self, in_channel, reduction=16):
        super().__init__()
        
        # Channel Squeeze & Excitation
        self.cSE = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(in_channel, in_channel // reduction, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channel // reduction, in_channel, 1),
            nn.Sigmoid()
        )
        
        # Spatial Squeeze & Excitation
        self.sSE = nn.Sequential(
            nn.Conv2d(in_channel, 1, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        # Channel SE
        cse = self.cSE(x) * x
        
        # Spatial SE  
        sse = self.sSE(x) * x
        
        # Concurrent - take element-wise maximum
        return torch.max(cse, sse)


class CoordAtt(nn.Module):
    def __init__(self, inp, oup, reduction=32):
        super(CoordAtt, self).__init__()
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))

        mip = max(8, inp // reduction)

        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)
        self.bn1 = nn.BatchNorm2d(mip)
        self.act = h_swish()

        self.conv_h = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)
        self.conv_w = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)
        self.conv_end = nn.Conv2d(oup, oup, kernel_size=1, stride=1, padding=0)
        self.self_SA_Enhance = SA_Enhance()

    def forward(self, rgb):
        x = rgb

        n, c, h, w = x.size()
        x_h = self.pool_h(x)
        x_w = self.pool_w(x).permute(0, 1, 3, 2)

        y = torch.cat([x_h, x_w], dim=2)
        y = self.conv1(y)
        y = self.bn1(y)
        y = self.act(y) # å‡è®¾æ˜¯[n, mip, h+w, 1]

        x_h, x_w = torch.split(y, [h, w], dim=2) # x_h: [n, mip, h, 1], [n, mip, w, 1]
        x_w = x_w.permute(0, 1, 3, 2)

        a_h = self.conv_h(x_h).sigmoid()
        a_w = self.conv_w(x_w).sigmoid()

        out_ca = x * a_w * a_h
        out_sa = self.self_SA_Enhance(out_ca)
        out = x.mul(out_sa)
        out = self.conv_end(out)

        return out


class h_sigmoid(nn.Module):
    """Hard Sigmoidæ¿€æ´»å‡½æ•°"""
    def __init__(self, inplace=True):
        super().__init__()
        self.relu = nn.ReLU6(inplace=inplace)

    def forward(self, x):
        return self.relu(x + 3) / 6


class h_swish(nn.Module):
    """Hard Swishæ¿€æ´»å‡½æ•°"""
    def __init__(self, inplace=True):
        super().__init__()
        self.sigmoid = h_sigmoid(inplace=inplace)

    def forward(self, x):
        return x * self.sigmoid(x)


class SA_Enhance(nn.Module):
    def __init__(self, kernel_size=7):
        super(SA_Enhance, self).__init__()

        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
        padding = 3 if kernel_size == 7 else 1

        self.conv1 = nn.Conv2d(1, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = max_out
        x = self.conv1(x)
        return self.sigmoid(x)



class DSAMBlock(nn.Module):
    """
    Dual-domain Strip Attention Module (DSAM)
    ç®€åŒ–ç‰ˆæœ¬ï¼Œç”¨äºMHSIUå¢å¼º
    """
    def __init__(self, in_channel):
        super().__init__()
        self.in_channel = in_channel
        
        # ç«‹æ–¹æ³¨æ„åŠ›ç»„ä»¶
        self.cubic_attention = nn.Sequential(
            nn.Conv2d(in_channel, in_channel // 4, 1),
            nn.BatchNorm2d(in_channel // 4),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channel // 4, in_channel, 1),
            nn.Sigmoid()
        )
        
        # æ¡å¸¦æ³¨æ„åŠ›ç»„ä»¶
        self.strip_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, None)),  # æ°´å¹³æ¡å¸¦
            nn.Conv2d(in_channel, in_channel, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        # ç«‹æ–¹æ³¨æ„åŠ›
        cubic_att = self.cubic_attention(x)
        
        # æ¡å¸¦æ³¨æ„åŠ›  
        strip_att = self.strip_attention(x)
        strip_att = strip_att.expand_as(x)
        
        # èåˆä¸¤ç§æ³¨æ„åŠ›
        enhanced = x * cubic_att * strip_att
        
        return enhanced


# ===============================================================================
# å¢å¼ºçš„MHSIUæ¨¡å—
# ===============================================================================

class EnhancedMHSIU(nn.Module):
    """
    å¢å¼ºçš„å¤šå±‚æ¬¡å°ºåº¦é›†æˆå•å…ƒï¼ˆEnhanced Multi-Hierarchical Scale Integration Unitï¼‰
    
    æ”¯æŒçš„æ³¨æ„åŠ›ç±»å‹ï¼š
    - 'scSE': å¹¶å‘ç©ºé—´é€šé“æŒ¤å‹æ¿€åŠ±æ³¨æ„åŠ›
    - 'coord': åæ ‡æ³¨æ„åŠ›  
    - 'hybrid': æ··åˆæ³¨æ„åŠ›ï¼ˆscSE + CoordAttï¼‰
    - 'original': åŸå§‹å·ç§¯æ³¨æ„åŠ›
    """
    
    def __init__(self, in_dim, num_groups=4, attention_type='scSE'):
        super().__init__()
        
        # ä¿æŒåŸæœ‰çš„åŸºç¡€ç»“æ„
        self.conv_l_pre = ConvBNReLU(in_dim, in_dim, 3, 1, 1)
        self.conv_s_pre = ConvBNReLU(in_dim, in_dim, 3, 1, 1)
        self.conv_l = ConvBNReLU(in_dim, in_dim, 3, 1, 1)
        self.conv_m = ConvBNReLU(in_dim, in_dim, 3, 1, 1)
        self.conv_s = ConvBNReLU(in_dim, in_dim, 3, 1, 1)
        
        self.conv_lms = ConvBNReLU(3 * in_dim, 3 * in_dim, 1)
        self.initial_merge = ConvBNReLU(3 * in_dim, 3 * in_dim, 1)

        self.num_groups = num_groups
        self.attention_type = attention_type

        # ğŸ”§ æ ¸å¿ƒæ”¹è¿›ï¼šæ›¿æ¢åŸæœ‰çš„å·ç§¯æ³¨æ„åŠ›ä¸ºæ›´å…ˆè¿›çš„æ³¨æ„åŠ›æœºåˆ¶
        if attention_type == 'scSE':
            # æ–¹æ¡ˆ1ï¼šä½¿ç”¨scSEæ³¨æ„åŠ› - æ¨èæ–¹æ¡ˆ
            self.attention_module = scSE(3 * in_dim // num_groups, reduction=4)
            self.trans = nn.Sequential(
                self.attention_module,  # ğŸ”§ å…ˆåº”ç”¨scSEæ³¨æ„åŠ›
                ConvBNReLU(3 * in_dim // num_groups, in_dim // num_groups, 1),
                nn.Conv2d(in_dim // num_groups, 3, 1),
                nn.Softmax(dim=1),
            )
        elif attention_type == 'coord':
            # æ–¹æ¡ˆ2ï¼šä½¿ç”¨CoordAttæ³¨æ„åŠ›
            self.attention_module = CoordAtt(3 * in_dim // num_groups, 3 * in_dim // num_groups)
            self.trans = nn.Sequential(
                self.attention_module,  # ğŸ”§ å…ˆåº”ç”¨CoordAttæ³¨æ„åŠ›
                ConvBNReLU(3 * in_dim // num_groups, in_dim // num_groups, 1),
                nn.Conv2d(in_dim // num_groups, 3, 1),
                nn.Softmax(dim=1),
            )
        elif attention_type == 'hybrid':
            # æ–¹æ¡ˆ3ï¼šæ··åˆæ³¨æ„åŠ›ï¼ˆscSE + CoordAttï¼‰
            self.scse_module = scSE(3 * in_dim // num_groups, reduction=4)
            self.coordatt_module = CoordAtt(3 * in_dim // num_groups, 3 * in_dim // num_groups)
            # å­¦ä¹ ä¸¤ç§æ³¨æ„åŠ›çš„èåˆæƒé‡
            self.fusion_weight = nn.Parameter(torch.tensor(0.5))
            self.trans = nn.Sequential(
                ConvBNReLU(3 * in_dim // num_groups, in_dim // num_groups, 1),
                nn.Conv2d(in_dim // num_groups, 3, 1),
                nn.Softmax(dim=1),
            )
        elif attention_type == 'msca':
            # æ–¹æ¡ˆ4ï¼šä½¿ç”¨MSCAå¤šå°ºåº¦å·ç§¯æ³¨æ„åŠ›
            try:
                from .advanced_enhanced_layers import MSCA_AttentionModule
                self.attention_module = MSCA_AttentionModule(3 * in_dim // num_groups)
                self.trans = nn.Sequential(
                    self.attention_module,  # ğŸ”§ å…ˆåº”ç”¨MSCAæ³¨æ„åŠ›
                    ConvBNReLU(3 * in_dim // num_groups, in_dim // num_groups, 1),
                    nn.Conv2d(in_dim // num_groups, 3, 1),
                    nn.Softmax(dim=1),
                )
            except ImportError:
                print("è­¦å‘Š: MSCAæ¨¡å—ä¸å¯ç”¨ï¼Œå›é€€åˆ°åŸå§‹æ³¨æ„åŠ›")
                self.trans = nn.Sequential(
                    ConvBNReLU(3 * in_dim // num_groups, in_dim // num_groups, 1),
                    nn.Conv2d(in_dim // num_groups, 3, 1),
                    nn.Softmax(dim=1),
                )
        elif attention_type == 'scsa':
            # æ–¹æ¡ˆ5ï¼šä½¿ç”¨SCSAç©ºé—´é€šé“ååŒæ³¨æ„åŠ›
            try:
                from .advanced_enhanced_layers import SCSA_Simplified
                self.attention_module = SCSA_Simplified(3 * in_dim // num_groups)
                self.trans = nn.Sequential(
                    self.attention_module,  # ğŸ”§ å…ˆåº”ç”¨SCSAæ³¨æ„åŠ›
                    ConvBNReLU(3 * in_dim // num_groups, in_dim // num_groups, 1),
                    nn.Conv2d(in_dim // num_groups, 3, 1),
                    nn.Softmax(dim=1),
                )
            except ImportError:
                print("è­¦å‘Š: SCSAæ¨¡å—ä¸å¯ç”¨ï¼Œå›é€€åˆ°åŸå§‹æ³¨æ„åŠ›")
                self.trans = nn.Sequential(
                    ConvBNReLU(3 * in_dim // num_groups, in_dim // num_groups, 1),
                    nn.Conv2d(in_dim // num_groups, 3, 1),
                    nn.Softmax(dim=1),
                )
        elif attention_type == 'original':
            # ä¿æŒåŸå§‹çš„å·ç§¯æ³¨æ„åŠ›
            self.trans = nn.Sequential(
                ConvBNReLU(3 * in_dim // num_groups, in_dim // num_groups, 1),
                nn.Conv2d(in_dim // num_groups, 3, 1),
                nn.Softmax(dim=1),
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ³¨æ„åŠ›ç±»å‹: {attention_type}ã€‚æ”¯æŒçš„ç±»å‹: 'scSE', 'coord', 'hybrid', 'msca', 'scsa', 'original'")

    def forward(self, l, m, s):
        # åŸæœ‰çš„å°ºåº¦å¯¹é½é€»è¾‘ä¿æŒä¸å˜
        tgt_size = s.shape[2:]
        l = self.conv_l_pre(l)
        l = F.adaptive_max_pool2d(l, tgt_size) + F.adaptive_avg_pool2d(l, tgt_size)
        m = self.conv_s_pre(m)
        m = F.adaptive_max_pool2d(m, tgt_size) + F.adaptive_avg_pool2d(m, tgt_size)

        l = self.conv_l(l)
        m = self.conv_m(m)
        s = self.conv_s(s)
        lms = torch.cat([l, m, s], dim=1)

        # ğŸ”§ å¢å¼ºçš„æ³¨æ„åŠ›å¤„ç†
        attn = self.conv_lms(lms)
        attn = rearrange(attn, "bt (nb ng d) h w -> (bt ng) (nb d) h w", nb=3, ng=self.num_groups)
        
        if self.attention_type == 'hybrid':
            # æ··åˆæ³¨æ„åŠ›ï¼šåº”ç”¨ä¸¤ç§æ³¨æ„åŠ›å¹¶èåˆ
            scse_out = self.scse_module(attn)
            coord_out = self.coordatt_module(attn)
            
            # å­¦ä¹ æƒé‡èåˆ
            weight = torch.sigmoid(self.fusion_weight)
            attn = weight * scse_out + (1 - weight) * coord_out
            
            # åº”ç”¨æœ€ç»ˆå˜æ¢
            attn = self.trans(attn)
        else:
            # å•ä¸€æ³¨æ„åŠ›æˆ–åŸå§‹æ³¨æ„åŠ›
            attn = self.trans(attn)
        
        attn = attn.unsqueeze(dim=2)  # BTG,3,1,H,W

        # ç‰¹å¾èåˆé€»è¾‘ä¿æŒä¸å˜
        x = self.initial_merge(lms)
        x = rearrange(x, "bt (nb ng d) h w -> (bt ng) nb d h w", nb=3, ng=self.num_groups)
        x = (attn * x).sum(dim=1)
        x = rearrange(x, "(bt ng) d h w -> bt (ng d) h w", ng=self.num_groups)

        return x


class DSAM_EnhancedMHSIU(nn.Module):
    """
    ä½¿ç”¨DSAMæ³¨æ„åŠ›å¢å¼ºçš„MHSIU
    """
    
    def __init__(self, in_dim, num_groups=4):
        super().__init__()
        
        # ä¿æŒåŸæœ‰çš„åŸºç¡€ç»“æ„
        self.conv_l_pre = ConvBNReLU(in_dim, in_dim, 3, 1, 1)
        self.conv_s_pre = ConvBNReLU(in_dim, in_dim, 3, 1, 1)
        self.conv_l = ConvBNReLU(in_dim, in_dim, 3, 1, 1)
        self.conv_m = ConvBNReLU(in_dim, in_dim, 3, 1, 1)
        self.conv_s = ConvBNReLU(in_dim, in_dim, 3, 1, 1)
        
        self.conv_lms = ConvBNReLU(3 * in_dim, 3 * in_dim, 1)
        self.initial_merge = ConvBNReLU(3 * in_dim, 3 * in_dim, 1)

        self.num_groups = num_groups
        
        # ä½¿ç”¨DSAMæ³¨æ„åŠ›æ¨¡å—
        self.dsam_module = DSAMBlock(3 * in_dim // num_groups)
        
        self.trans = nn.Sequential(
            self.dsam_module,  # ğŸ”§ å…ˆåº”ç”¨DSAMæ³¨æ„åŠ›
            ConvBNReLU(3 * in_dim // num_groups, in_dim // num_groups, 1),
            nn.Conv2d(in_dim // num_groups, 3, 1),
            nn.Softmax(dim=1),
        )

    def forward(self, l, m, s):
        # åŸæœ‰çš„å°ºåº¦å¯¹é½é€»è¾‘
        tgt_size = s.shape[2:]
        l = self.conv_l_pre(l)
        l = F.adaptive_max_pool2d(l, tgt_size) + F.adaptive_avg_pool2d(l, tgt_size)
        m = self.conv_s_pre(m)
        m = F.adaptive_max_pool2d(m, tgt_size) + F.adaptive_avg_pool2d(m, tgt_size)

        l = self.conv_l(l)
        m = self.conv_m(m)
        s = self.conv_s(s)
        lms = torch.cat([l, m, s], dim=1)

        # DSAMå¢å¼ºçš„æ³¨æ„åŠ›å¤„ç†
        attn = self.conv_lms(lms)
        attn = rearrange(attn, "bt (nb ng d) h w -> (bt ng) (nb d) h w", nb=3, ng=self.num_groups)
        
        attn = self.trans(attn)
        attn = attn.unsqueeze(dim=2)

        # ç‰¹å¾èåˆ
        x = self.initial_merge(lms)
        x = rearrange(x, "bt (nb ng d) h w -> (bt ng) nb d h w", nb=3, ng=self.num_groups)
        x = (attn * x).sum(dim=1)
        x = rearrange(x, "(bt ng) d h w -> bt (ng d) h w", ng=self.num_groups)

        return x

# ===============================================================================
# æµ‹è¯•ä»£ç 
# ===============================================================================

if __name__ == "__main__":
    print("=== æµ‹è¯•å¢å¼ºMHSIUæ¨¡å— ===")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    l = torch.randn(2, 64, 36, 36)  # å¤§å°ºåº¦
    m = torch.randn(2, 64, 24, 24)  # ä¸­å°ºåº¦  
    s = torch.randn(2, 64, 12, 12)  # å°å°ºåº¦
    
    print("è¾“å…¥å½¢çŠ¶:")
    print(f"  å¤§å°ºåº¦ (l): {l.shape}")
    print(f"  ä¸­å°ºåº¦ (m): {m.shape}")
    print(f"  å°å°ºåº¦ (s): {s.shape}")
    
    # æµ‹è¯•ä¸åŒçš„æ³¨æ„åŠ›ç±»å‹
    attention_types = ['scSE', 'coord', 'hybrid', 'original']
    
    for attention_type in attention_types:
        print(f"\næµ‹è¯• {attention_type} æ³¨æ„åŠ›:")
        
        model = EnhancedMHSIU(64, num_groups=4, attention_type=attention_type)
        
        with torch.no_grad():
            output = model(l, m, s)
            print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(f"  å‚æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    # æµ‹è¯•DSAMå¢å¼º
    print(f"\næµ‹è¯• DSAM æ³¨æ„åŠ›:")
    model_dsam = DSAM_EnhancedMHSIU(64, num_groups=4)
    
    with torch.no_grad():
        output_dsam = model_dsam(l, m, s)
        print(f"  è¾“å‡ºå½¢çŠ¶: {output_dsam.shape}")
        print(f"  å‚æ•°é‡: {sum(p.numel() for p in model_dsam.parameters() if p.requires_grad):,}")
    
    print("\n=== MultiHeadMyNet æ¶ˆèå®éªŒæµ‹è¯• ===")
    
    # ğŸ”§ æ¶ˆèå®éªŒ1: å¤´æ•°æ¶ˆè
    print("\n1. å¤´æ•°æ¶ˆèå®éªŒ:")
    head_nums = [0, 1, 2, 6, 8]
    for num_heads in head_nums:
        model = MultiHeadMyNet(64, num_groups=3, num_heads=num_heads)
        with torch.no_grad():
            output = model(l, m, s)
            params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"  å¤´æ•°={num_heads}: è¾“å‡º{output.shape}, å‚æ•°é‡={params:,}")
    
    # ğŸ”§ æ¶ˆèå®éªŒ2: é—¨æ§æ–¹å¼æ¶ˆè
    print("\n2. é—¨æ§æ–¹å¼æ¶ˆèå®éªŒ:")
    gate_types = ['softmax', 'sigmoid', 'none']
    for gate_type in gate_types:
        model = MultiHeadMyNet(64, num_groups=3, num_heads=4, gate_type=gate_type)
        with torch.no_grad():
            output = model(l, m, s)
            params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"  é—¨æ§={gate_type}: è¾“å‡º{output.shape}, å‚æ•°é‡={params:,}")
    
    # ğŸ”§ æ¶ˆèå®éªŒ3: å¤´é—´èåˆæ–¹å¼æ¶ˆè
    print("\n3. å¤´é—´èåˆæ–¹å¼æ¶ˆèå®éªŒ:")
    fusion_types = ['conv', 'weighted_sum']
    for fusion_type in fusion_types:
        model = MultiHeadMyNet(64, num_groups=3, num_heads=4, fusion_type=fusion_type)
        with torch.no_grad():
            output = model(l, m, s)
            params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"  èåˆ={fusion_type}: è¾“å‡º{output.shape}, å‚æ•°é‡={params:,}")
    
    # ğŸ”§ æ¶ˆèå®éªŒ4: å½’ä¸€åŒ–ä¸æ¿€æ´»æ¶ˆè
    print("\n4. å½’ä¸€åŒ–ä¸æ¿€æ´»æ¶ˆèå®éªŒ:")
    norm_types = ['bn', 'gn']
    for norm_type in norm_types:
        model = MultiHeadMyNet(64, num_groups=3, num_heads=4, norm_type=norm_type)
        with torch.no_grad():
            output = model(l, m, s)
            params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            activation = 'ReLU' if norm_type == 'bn' else 'SiLU'
            print(f"  å½’ä¸€åŒ–={norm_type.upper()}+{activation}: è¾“å‡º{output.shape}, å‚æ•°é‡={params:,}")
    
    # ğŸ”§ ç»„åˆæ¶ˆèå®éªŒç¤ºä¾‹
    print("\n5. ç»„åˆæ¶ˆèå®éªŒç¤ºä¾‹:")
    ablation_configs = [
        {'num_heads': 4, 'gate_type': 'softmax', 'fusion_type': 'conv', 'norm_type': 'bn'},
        {'num_heads': 6, 'gate_type': 'sigmoid', 'fusion_type': 'weighted_sum', 'norm_type': 'gn'},
        {'num_heads': 2, 'gate_type': 'none', 'fusion_type': 'conv', 'norm_type': 'bn'},
    ]
    
    for i, config in enumerate(ablation_configs, 1):
        model = MultiHeadMyNet(64, num_groups=3, **config)
        with torch.no_grad():
            output = model(l, m, s)
            params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"  é…ç½®{i} {config}: è¾“å‡º{output.shape}, å‚æ•°é‡={params:,}")
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("ğŸ“Œ æ¨èä½¿ç”¨ attention_type='scSE' ä½œä¸ºé»˜è®¤é…ç½®")
    print("ğŸ“Œ æ··åˆæ³¨æ„åŠ› 'hybrid' å¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œä½†è®¡ç®—å¼€é”€ç¨å¤§")
    print("ğŸ“Œ MultiHeadMyNet æ¶ˆèå®éªŒå·²æ·»åŠ ï¼Œæ”¯æŒå¤´æ•°ã€é—¨æ§ã€èåˆã€å½’ä¸€åŒ–å››ä¸ªç»´åº¦çš„å¯¹æ¯”")
